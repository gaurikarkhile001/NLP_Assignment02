{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW (Count): Counter({'love': 2, 'football': 2, 'playing': 1, 'great': 1, 'sport': 1, 'watching': 1, 'movies': 1})\n",
            "Normalized BoW: {'love': 0.2222222222222222, 'playing': 0.1111111111111111, 'football': 0.2222222222222222, 'great': 0.1111111111111111, 'sport': 0.1111111111111111, 'watching': 0.1111111111111111, 'movies': 0.1111111111111111}\n",
            "TF-IDF for doc 1: {'love': 0.0, 'playing': 0.13515503603605478, 'football': 0.0}\n",
            "TF-IDF for doc 2: {'football': 0.0, 'great': 0.13515503603605478, 'sport': 0.13515503603605478}\n",
            "TF-IDF for doc 3: {'love': 0.0, 'watching': 0.13515503603605478, 'movies': 0.13515503603605478}\n",
            "Word2Vec vector for 'football': [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import math\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample documents\n",
        "docs = [\n",
        "    \"I love playing football\",\n",
        "    \"Football is a great sport\",\n",
        "    \"I love watching movies\"\n",
        "]\n",
        "\n",
        "# Preprocessing and Tokenization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized_docs = []\n",
        "for doc in docs:\n",
        "    words = word_tokenize(doc.lower())\n",
        "    filtered = [w for w in words if w.isalpha() and w not in stop_words]\n",
        "    tokenized_docs.append(filtered)\n",
        "\n",
        "# 1. Bag of Words (Count)\n",
        "all_words = [word for doc in tokenized_docs for word in doc]\n",
        "word_counts = Counter(all_words)\n",
        "print(\"BoW (Count):\", word_counts)\n",
        "\n",
        "# 2. Normalized Count (TF)\n",
        "total_words = sum(word_counts.values())\n",
        "normalized_bow = {word: count/total_words for word, count in word_counts.items()}\n",
        "print(\"Normalized BoW:\", normalized_bow)\n",
        "\n",
        "# 3. TF-IDF\n",
        "def compute_tf(doc):\n",
        "    tf = Counter(doc)\n",
        "    total = len(doc)\n",
        "    return {word: freq/total for word, freq in tf.items()}\n",
        "\n",
        "def compute_idf(docs):\n",
        "    N = len(docs)\n",
        "    idf = {}\n",
        "    all_words = set(word for doc in docs for word in doc)\n",
        "    for word in all_words:\n",
        "        containing = sum(1 for doc in docs if word in doc)\n",
        "        idf[word] = math.log(N / (1 + containing))\n",
        "    return idf\n",
        "\n",
        "def compute_tfidf(doc, idf):\n",
        "    tf = compute_tf(doc)\n",
        "    return {word: tf[word] * idf[word] for word in doc}\n",
        "\n",
        "idf = compute_idf(tokenized_docs)\n",
        "for i, doc in enumerate(tokenized_docs):\n",
        "    tfidf = compute_tfidf(doc, idf)\n",
        "    print(f\"TF-IDF for doc {i+1}:\", tfidf)\n",
        "\n",
        "# 4. Word2Vec using gensim\n",
        "model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=2, min_count=1)\n",
        "print(\"Word2Vec vector for 'football':\", model.wv['football'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
