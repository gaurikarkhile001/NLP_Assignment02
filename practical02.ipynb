import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import math
from gensim.models import Word2Vec

nltk.download('punkt')
nltk.download('stopwords')

# Sample documents
docs = [
    "I love playing football",
    "Football is a great sport",
    "I love watching movies"
]

# Preprocessing and Tokenization
stop_words = set(stopwords.words('english'))
tokenized_docs = []
for doc in docs:
    words = word_tokenize(doc.lower())
    filtered = [w for w in words if w.isalpha() and w not in stop_words]
    tokenized_docs.append(filtered)

# 1. Bag of Words (Count)
all_words = [word for doc in tokenized_docs for word in doc]
word_counts = Counter(all_words)
print("BoW (Count):", word_counts)

# 2. Normalized Count (TF)
total_words = sum(word_counts.values())
normalized_bow = {word: count/total_words for word, count in word_counts.items()}
print("Normalized BoW:", normalized_bow)

# 3. TF-IDF
def compute_tf(doc):
    tf = Counter(doc)
    total = len(doc)
    return {word: freq/total for word, freq in tf.items()}

def compute_idf(docs):
    N = len(docs)
    idf = {}
    all_words = set(word for doc in docs for word in doc)
    for word in all_words:
        containing = sum(1 for doc in docs if word in doc)
        idf[word] = math.log(N / (1 + containing))
    return idf

def compute_tfidf(doc, idf):
    tf = compute_tf(doc)
    return {word: tf[word] * idf[word] for word in doc}

idf = compute_idf(tokenized_docs)
for i, doc in enumerate(tokenized_docs):
    tfidf = compute_tfidf(doc, idf)
    print(f"TF-IDF for doc {i+1}:", tfidf)

# 4. Word2Vec using gensim
model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=2, min_count=1)
print("Word2Vec vector for 'football':", model.wv['football'])
